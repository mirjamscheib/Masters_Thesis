---
title: "ML_modelling"
format: html
---

# Load packages
```{r}
library(mlrMBO) # Hyperparameter optimization 
library(gbm) # BRT 
install.packages("ranger")
library(ranger) # RF
library(dplyr)
library(raster)
library(readr)
```

# Load packages Geocomputation
```{r}
library(sf)
library(terra)
library(dplyr)
library(future)
library(lgr)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3spatiotempcv)
library(mlr3tuning)
library(mlr3viz)
library(progressr)
```


```{r}
library(sf)
library(terra)
library(dplyr)
library(future)
library(lgr)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3spatiotempcv)
library(mlr3tuning)
library(mlr3viz)
library(progressr)
```


# Load data 
## Own Data
```{r}
data <- read.csv("abiotic_mi_sampling/Lab_k_index.csv", sep = ",")

data_new <- data[ , c(2,3,4,7,9, 22, 23, 24, 27)]

data_new <- data_new |>
  mutate(Reach_Untersuchungsstelle = as.factor(Reach_Untersuchungsstelle),
         x = as.numeric(x))
```

# Book Exercise 
## Load Data
```{r}
# Load Data
data("lsl", "study_mask", package = "spDataLarge")
ta = terra::rast(system.file("raster/ta.tif", package = "spDataLarge"))

# Inspect Data - 175 landslide and 175 non-landslide points
summary(lsl$lslpts)

# look at the table 
head(lsl)
```

## Conventional Modelling Approach GLM 
```{r}
# make model
fit <- glm(lslpts ~ slope + cplan + cprof + elev + log10_carea,
          family = binomial(),
          data = lsl)

# look at model output
summary(fit)
fit
```

The model object fit, of class glm, contains the coefficients defining the fitted relationship between response and predictors. It can also be used for prediction. This is done with the generic predict().
Setting type to response returns the predicted probabilities (of landslide occurrence) for each observation in lsl, as illustrated below.

```{r}
pred_glm <- predict(object = fit, type = "response")
head(pred_glm)
#>      1      2      3      4      5      6 
#> 0.1901 0.1172 0.0952 0.2503 0.3382 0.1575
```

```{r}
# making the prediction
pred <- terra::predict(ta, model = fit, type = "response")

plot(pred)
ta # raster hat schon alle eigenschaften, die auch im modell sind... 
```

## Calculate AUROC
```{r}
#  takes the response and the predicted values as inputs

pROC::auc(pROC::roc(lsl$lslpts, fitted(fit)))
#> Area under the curve: 0.8216
```

## Spatial CV with MLR3 
### GLM - create a task
```{r}
# create task
task <- mlr3spatiotempcv::TaskClassifST$new(
  id = "ecuador_lsl",
  backend = mlr3::as_data_backend(lsl), #  input data includes the response and predictor variables
  target = "lslpts", # name of a response variable (in our case this is lslpts) 
  positive = "TRUE", # positive determines which of the two factor levels of the response variable indicate the landslide initiation point (in our case this is TRUE)
  coordinate_names = c("x", "y"), # names of the coordinate columns
  coords_as_features = FALSE, # decide if we want to use the coordinates as predictors in the modeling
  crs = "EPSG:32717" # coordinate system 
  )
```

For a short data exploration, the autoplot() function of the mlr3viz package might come in handy since it plots the response against all predictors and all predictors against all predictors (not shown).

```{r}
# plot response against each predictor
mlr3viz::autoplot(task, type = "duo")
# plot all variables against each other
mlr3viz::autoplot(task, type = "pairs")
```

### GLM - choose learner
Having created a task, we need to choose a learner that determines the statistical learning method to use. All classification learners start with classif. and all regression learners with regr. (see ?Learner for details).
```{r}
# kein Paket namens ‘dictionar6’
# Access all learners
mlr3extralearners::list_mlr3learners()

# Filter learners for your specific problem 
mlr3extralearners::list_mlr3learners(
  filter = list(class = "classif", properties = "twoclass"), 
  select = c("id", "mlr3_package", "required_packages")) |>
  head()
```

This yields all learners able to model two-class problems (landslide yes or no). We opt for the binomial classification method used in Section 12.3 and implemented as classif.log_reg in mlr3learners. Additionally, we need to specify the predict.type which determines the type of the prediction with prob resulting in the predicted probability for landslide occurrence between 0 and 1 (this corresponds to type = response in predict.glm).

```{r}
learner <- mlr3::lrn("classif.log_reg", predict_type = "prob")

# To access the help page of the learner and find out from which package it was taken, we can run:

learner$help()
```

### GLM - resampling method
We will use a 100-repeated 5-fold spatial CV: five partitions will be chosen based on the provided coordinates in our task and the partitioning will be repeated 100 times:

```{r}
resampling <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)

# reduce verbosity
lgr::get_logger("mlr3")$set_threshold("warn")
# run spatial cross-validation and save it to resample result glm (rr_glm)
rr_spcv_glm <- mlr3::resample(task = task,
                             learner = learner,
                             resampling = resampling)
# compute the AUROC as a data.table
score_spcv_glm <- rr_spcv_glm$score(measure = mlr3::msr("classif.auc"))
# keep only the columns you need
score_spcv_glm <- score_spcv_glm[, .(task_id, learner_id, resampling_id, 
                                    classif.auc)]
```

```{r}
score <- readRDS("extdata/12-bmr_score.rds")
score_spcv_glm <- score[learner_id == "classif.log_reg" & 
                         resampling_id == "repeated_spcv_coords"]
```

```{r}
mean(score_spcv_glm$classif.auc) |>
  round(2)
#> [1] 0.77
```

### Spatial tuning of ML
```{r}
# set up task 
lrn_ksvm <- mlr3::lrn("classif.ksvm", predict_type = "prob", kernel = "rbfdot",
                     type = "C-svc")
lrn_ksvm$fallback <- lrn("classif.featureless", predict_type = "prob")

# resampling strategy 
# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (tune_level object in the code chunk below; see Figure 12.6 for a visual representation). To find the optimal hyperparameter combination, we fit 50 models (terminator object in the code chunk below) in each of these subfolds with randomly selected values for the hyperparameters C and Sigma. The random selection of values C and Sigma is additionally restricted to a predefined tuning space (search_space object). The range of the tuning space was chosen with values recommended in the literature (Schratz et al. 2019).

```{r}
# five spatially disjoint partitions
tune_level <- mlr3::rsmp("spcv_coords", folds = 5)
# use 50 randomly selected hyperparameters
terminator <- mlr3tuning::trm("evals", n_evals = 50)
tuner <- mlr3tuning::tnr("random_search")
# define the outer limits of the randomly selected hyperparameters
search_space <- paradox::ps(
  C = paradox::p_dbl(lower = -12, upper = 15, trafo = function(x) 2^x),
  sigma = paradox::p_dbl(lower = -15, upper = 6, trafo = function(x) 2^x)
)
```

The next stage is to modify the learner lrn_ksvm in accordance with all the characteristics defining the hyperparameter tuning with AutoTuner$new().

```{r}
at_ksvm <- mlr3tuning::AutoTuner$new(
  learner = lrn_ksvm,
  resampling = tune_level,
  measure = mlr3::msr("classif.auc"),
  search_space = search_space,
  terminator = terminator,
  tuner = tuner
)
```

The tuning is now set-up to fit 250 models to determine optimal hyperparameters for one fold. Repeating this for each fold, we end up with 1,250 (250 * 5) models for each repetition. Repeated 100 times means fitting a total of 125,000 models to identify optimal hyperparameters (Figure 12.3). 

```{r}
library(future)
# execute the outer loop sequentially and parallelize the inner loop
future::plan(list("sequential", "multisession"), 
             workers = floor(availableCores() / 2))
```

Now we are set up for computing the nested spatial CV. Specifying the resample() parameters follows the exact same procedure as presented when using a GLM, the only difference being the store_models and encapsulate arguments. Setting the former to TRUE would allow the extraction of the hyperparameter tuning results which is important if we plan follow-up analyses on the tuning. The latter ensures that the processing continues even if one of the models throws an error. 

```{r}
progressr::with_progress(expr = {
  rr_spcv_svm = mlr3::resample(task = task,
                               learner = at_ksvm, 
                               # outer resampling (performance level)
                               resampling = perf_level,
                               store_models = FALSE,
                               encapsulate = "evaluate")
})

# stop parallelization
future:::ClusterRegistry("stop")
# compute the AUROC values
score_spcv_svm <- rr_spcv_svm$score(measure = mlr3::msr("classif.auc")) 
# keep only the columns you need
score_spcv_svm <- score_spcv_svm[, .(task_id, learner_id, resampling_id, classif.auc)]
```


```{r}
# final mean AUROC
round(mean(score_spcv_svm$classif.auc), 2)
#> [1] 0.74
```
Please note also that using more than 50 iterations in the random search of the SVM would probably yield hyperparameters that result in models with a better AUROC (Schratz et al. 2019). On the other hand, increasing the number of random search iterations would also increase the total number of models and thus runtime.
For predictive mapping purposes, one would tune the hyperparameters on the complete dataset. This will be covered in Chapter 15.

# Hyperparameter optimization 
## BRT
```{r}
# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]

# At first, n hyperparameter settings are randomly chosen from a user-defined search space
brt <- gbm(k_index ~., data = train_data, distribution = "gaussian", n.trees = 100, interaction.depth = 1, shrinkage = 0.1, n.minobsinnode = 1)


```

## RF
```{r}

```

