---
title: "ML_modelling"
format: html
---

# Load packages
```{r}
library(mlrMBO) # Hyperparameter optimization 
library(gbm) # BRT 
install.packages("ranger")
library(ranger) # RF
library(dplyr)
library(raster)
library(readr)
```

# Load packages Geocomputation
```{r}
library(sf)
library(terra)
library(dplyr)
library(future)
library(lgr)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3spatiotempcv)
library(mlr3tuning)
library(mlr3viz)
library(progressr)
```


```{r}
library(sf)
library(terra)
library(dplyr)
library(future)
library(lgr)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(mlr3spatiotempcv)
library(mlr3tuning)
library(mlr3viz)
library(progressr)
```


# Load data 
## Own Data
```{r}
data <- read.csv("abiotic_mi_sampling/Lab_k_index.csv", sep = ",")

data_new <- data[ , c(2,3,4,7,9, 22, 23, 24, 27)]

data_new <- data_new |>
  mutate(Reach_Untersuchungsstelle = as.factor(Reach_Untersuchungsstelle),
         x = as.numeric(x))

head(data_new)
```

## Spatial CV with MLR3 
### Spatial tuning of ML
#### Random Forest 
```{r}
mlr_learners$get("regr.ranger")
lrn("regr.ranger")

# set up task 
lrn_rf <- mlr3::lrn("regr.ranger", predict_type = "response", num.trees = to_tune(c(100, 200, 400)))

# predict_type = "response" or "se"

lrn_ksvm$fallback <- lrn("classif.featureless", predict_type = "prob")

# resampling strategy 
# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

This means that we split each fold again into five spatially disjoint subfolds which are used to determine the optimal hyperparameters (tune_level object in the code chunk below; see Figure 12.6 for a visual representation). To find the optimal hyperparameter combination, we fit 50 models (terminator object in the code chunk below) in each of these subfolds with randomly selected values for the hyperparameters C and Sigma. The random selection of values C and Sigma is additionally restricted to a predefined tuning space (search_space object). The range of the tuning space was chosen with values recommended in the literature (Schratz et al. 2019).

```{r}
# five spatially disjoint partitions
tune_level <- mlr3::rsmp("spcv_coords", folds = 5)
# use 50 randomly selected hyperparameters
terminator <- mlr3tuning::trm("evals", n_evals = 50)
tuner <- mlr3tuning::tnr("random_search")
# define the outer limits of the randomly selected hyperparameters
search_space <- paradox::ps(
  C = paradox::p_dbl(lower = -12, upper = 15, trafo = function(x) 2^x),
  sigma = paradox::p_dbl(lower = -15, upper = 6, trafo = function(x) 2^x)
)
```

The next stage is to modify the learner lrn_ksvm in accordance with all the characteristics defining the hyperparameter tuning with AutoTuner$new().

```{r}
at_ksvm <- mlr3tuning::AutoTuner$new(
  learner = lrn_ksvm,
  resampling = tune_level,
  measure = mlr3::msr("classif.auc"),
  search_space = search_space,
  terminator = terminator,
  tuner = tuner
)
```

The tuning is now set-up to fit 250 models to determine optimal hyperparameters for one fold. Repeating this for each fold, we end up with 1,250 (250 * 5) models for each repetition. Repeated 100 times means fitting a total of 125,000 models to identify optimal hyperparameters (Figure 12.3). 

```{r}
library(future)
# execute the outer loop sequentially and parallelize the inner loop
future::plan(list("sequential", "multisession"), 
             workers = floor(availableCores() / 2))
```

Now we are set up for computing the nested spatial CV. Specifying the resample() parameters follows the exact same procedure as presented when using a GLM, the only difference being the store_models and encapsulate arguments. Setting the former to TRUE would allow the extraction of the hyperparameter tuning results which is important if we plan follow-up analyses on the tuning. The latter ensures that the processing continues even if one of the models throws an error. 

```{r}
progressr::with_progress(expr = {
  rr_spcv_svm = mlr3::resample(task = task,
                               learner = at_ksvm, 
                               # outer resampling (performance level)
                               resampling = perf_level,
                               store_models = FALSE,
                               encapsulate = "evaluate")
})

# stop parallelization
future:::ClusterRegistry("stop")
# compute the AUROC values
score_spcv_svm <- rr_spcv_svm$score(measure = mlr3::msr("classif.auc")) 
# keep only the columns you need
score_spcv_svm <- score_spcv_svm[, .(task_id, learner_id, resampling_id, classif.auc)]
```


```{r}
# final mean AUROC
round(mean(score_spcv_svm$classif.auc), 2)
#> [1] 0.74
```
Please note also that using more than 50 iterations in the random search of the SVM would probably yield hyperparameters that result in models with a better AUROC (Schratz et al. 2019). On the other hand, increasing the number of random search iterations would also increase the total number of models and thus runtime.
For predictive mapping purposes, one would tune the hyperparameters on the complete dataset. This will be covered in Chapter 15.

# Hyperparameter optimization 
## BRT
```{r}
# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]

# At first, n hyperparameter settings are randomly chosen from a user-defined search space
brt <- gbm(k_index ~., data = train_data, distribution = "gaussian", n.trees = 100, interaction.depth = 1, shrinkage = 0.1, n.minobsinnode = 1)


```

## RF
```{r}

```

