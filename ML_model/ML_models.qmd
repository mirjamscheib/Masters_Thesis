---
title: "ML_models"
format: html
---

# Libraries 
```{r}
#clear R environment
rm(list = ls())

# check for a package, install and load 
pkgCheck <- function(x){ 
  if (!require(x,character.only = TRUE)){
    install.packages(x,dependencies=TRUE)
    if(!require(x,character.only = TRUE)) {
      stop()
    }
  }
}

pkgCheck("sf")
pkgCheck("terra")
pkgCheck("dplyr")
pkgCheck("data.table")  # fast data.frame manipulation (used by mlr3)
pkgCheck("mlr3") # machine learning (see Chapter 12)
pkgCheck("mlr3spatiotempcv") # spatio-temporal resampling 
pkgCheck("mlr3tuning") # hyperparameter tuning package
pkgCheck("mlr3learners") # interface to most important machine learning packages
pkgCheck("paradox") # defining hyperparameter spaces
pkgCheck("ranger")  # rf package
pkgCheck("tree") # decision tree package
pkgCheck("readr")
pkgCheck("sp")
pkgCheck("raster")
pkgCheck("gbm")
pkgCheck("xgboost")
pkgCheck("caret") # for cohen's kappa calculation 
pkgCheck("pdp")
pkgCheck("gbm")
```

einfachste Variante die mÃ¶glich ist test und train dataset zu trennen (hydrodyn. modelle)

# Load data 
```{r}
# Load data containing only v, and v + wd explanatory variables
data_v <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_v.csv") 
data_wd <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_wd_v.csv") 

# w/o hydr. models "GL1", "GL2", "L2", "M1", "S1", "S2", "TH4", "VR3"
# training set containing velocity 
training_v <- data_v |>
  filter(Reach_Untersuchungsstelle != "GL1",
         Reach_Untersuchungsstelle != "GL2",         
         Reach_Untersuchungsstelle != "L2",
         Reach_Untersuchungsstelle != "M1",
         Reach_Untersuchungsstelle != "S1",
         Reach_Untersuchungsstelle != "S2",
         Reach_Untersuchungsstelle != "TH4", # noch nicht im lab datensatz, darum wirds noch nicht eingerechnet
         Reach_Untersuchungsstelle != "VR3")

# training set containing velocity + wd
training_wd <- data_wd |>
  filter(Reach_Untersuchungsstelle != "GL1",
         Reach_Untersuchungsstelle != "GL2",         
         Reach_Untersuchungsstelle != "L2",
         Reach_Untersuchungsstelle != "M1",
         Reach_Untersuchungsstelle != "S1",
         Reach_Untersuchungsstelle != "S2",
         Reach_Untersuchungsstelle != "TH4", # noch nicht im lab datensatz, darum wirds noch nicht eingerechnet
         Reach_Untersuchungsstelle != "VR3")

# w/ hydr. models 
# test set containing velocity 
test_v <- data_v |>
  filter(Reach_Untersuchungsstelle == "GL1" |
         Reach_Untersuchungsstelle == "GL2" |       
         Reach_Untersuchungsstelle == "L2" |
         Reach_Untersuchungsstelle == "M1" |
         Reach_Untersuchungsstelle == "S1" |
         Reach_Untersuchungsstelle == "S2" |
         Reach_Untersuchungsstelle == "TH4" | # fehlt noch im lab datensatz 
         Reach_Untersuchungsstelle == "VR3")

# test set containing velocity + wd
test_wd <- data_wd |>
  filter(Reach_Untersuchungsstelle == "GL1" |
         Reach_Untersuchungsstelle == "GL2" |       
         Reach_Untersuchungsstelle == "L2" |
         Reach_Untersuchungsstelle == "M1" |
         Reach_Untersuchungsstelle == "S1" |
         Reach_Untersuchungsstelle == "S2" |
         Reach_Untersuchungsstelle == "TH4" | # fehlt noch im lab datensatz 
         Reach_Untersuchungsstelle == "VR3")

# Prep data to model 
# TRAINING SETS
# With k_index as response variable & velocity 
training_v_k <- training_v |>
  dplyr::select(k_index, velocity, x, y)

# With shannon index as response variable & velocity 
training_v_s <- training_v |>
  dplyr::select(shannon, velocity, x, y)

# With abundance as response variable & velocity 
training_v_a <- training_v |>
  dplyr::select(abundance, velocity, x, y)

# With k_index as response variable & velocity + wd
training_wd_k <- training_wd |>
  dplyr::select(k_index, velocity, wd_cm, x, y)

# With shannon index as response variable & velocity + wd
training_wd_s <- training_wd |>
  dplyr::select(shannon, velocity, wd_cm, x, y)

# With abundance index as response variable & velocity + wd
training_wd_a <- training_wd |>
  dplyr::select(abundance, velocity, wd_cm, x, y)

# TEST SETS
# With k_index as response variable & velocity
test_v_k <- test_v |>
  dplyr::select(k_index, velocity, x, y) 

# With shannon index as response variable & velocity
test_v_s <- test_v |>
  dplyr::select(shannon, velocity, x, y) 

# With abundance as response variable & velocity
test_v_a <- test_v |>
  dplyr::select(abundance, velocity, x, y) 

# With k_index as response variable & velocity + wd
test_wd_k <- test_wd |>
  dplyr::select(k_index, velocity, wd_cm, x, y) 

# With shannon index as response variable & velocity + wd
test_wd_s <- test_wd |>
  dplyr::select(shannon, velocity, wd_cm, x, y) 

# With abundance as response variable & velocity + wd
test_wd_a <- test_wd |>
  dplyr::select(abundance, velocity, wd_cm, x, y) 

# convert dataframes into spatial objects for mlr3 spatial hyperparameter tuning 
# for k_index and velocity 
data_v_k_sp <-  st_as_sf(training_v_k, coords =  c("x", "y"))
st_crs(data_v_k_sp) <- "EPSG: 2056"

# for shannon index and velocity 
data_v_s_sp <-  st_as_sf(training_v_s, coords =  c("x", "y"))
st_crs(data_v_s_sp) <- "EPSG: 2056"

# for abundance and velocity 
data_v_a_sp <-  st_as_sf(training_v_a, coords =  c("x", "y"))
st_crs(data_v_a_sp) <- "EPSG: 2056"

# for k_index and velocity + wd
data_wd_k_sp <-  st_as_sf(training_wd_k, coords =  c("x", "y"))
st_crs(data_wd_k_sp) <- "EPSG: 2056"

# for shannon index and velocity + wd
data_wd_s_sp <-  st_as_sf(training_wd_s, coords =  c("x", "y"))
st_crs(data_wd_s_sp) <- "EPSG: 2056"

# for abundance and velocity + wd
data_wd_a_sp <-  st_as_sf(training_wd_a, coords =  c("x", "y"))
st_crs(data_wd_a_sp) <- "EPSG: 2056"
```


# Random Forest 
## Create Task & Learner
```{r}
# ONLY VELOCITY AS PREDICTOR (& x, y for spatial hyperparameter tuning)
# create task for k_index and velocity
task_rf_v_k <- mlr3spatiotempcv::as_task_regr_st(data_v_k_sp,
  id = "velocity", target = "k_index")

# create task for shannon index and velocity
task_rf_v_s <- mlr3spatiotempcv::as_task_regr_st(data_v_s_sp,
  id = "velocity", target = "shannon")

# create task for abundance and velocity
task_rf_v_a <- mlr3spatiotempcv::as_task_regr_st(data_v_a_sp,
  id = "velocity", target = "abundance")

# VELOCITY + WD AS PREDICTORS (& x, y for spatial hyperparameter tuning)
# create task for k_index and velocity + wd
task_rf_wd_k <- mlr3spatiotempcv::as_task_regr_st(data_wd_k_sp,
  id = "velocity", target = "k_index")

# create task for shannon index and velocity + wd
task_rf_wd_s <- mlr3spatiotempcv::as_task_regr_st(data_wd_s_sp,
  id = "velocity", target = "shannon")

# create task for abundance and velocity + wd
task_rf_wd_a <- mlr3spatiotempcv::as_task_regr_st(data_wd_a_sp,
  id = "velocity", target = "abundance")

# create learner for a random forest model from the ranger package
# for k_index, shannon index and abundance + velocity + wd valid
lrn_rf <- lrn("regr.ranger", predict_type = "response")

# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100) # wo brauche ich das??
```

## Specifying the search space
```{r}
# ONLY VELOCITY AS PREDICTOR
# specifying the search space for k_index and velocity
search_space_v_k <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_k$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for shannon index and velocity
search_space_v_s <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_s$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for abundance and velocity
search_space_v_a <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_a$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# VELOCITY + WD AS PREDICTORS 
# specifying the search space for k_index and velocity + wd
search_space_wd_k <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_k$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for shannon index and velocity + wd
search_space_wd_s <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_s$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for abundance and velocity + wd
search_space_wd_a <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_a$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# num.trees noch suchen? 
```

## Hyperparameter tuning 
```{r}
# VELOCITY AS PREDICTOR 
# with k_index and velocity 
autotuner_rf_v_k <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_k, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with shannon index and velocity 
autotuner_rf_v_s <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_s, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with abundance and velocity 
autotuner_rf_v_a <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_a, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# VELOCITY AND WD AS PREDICTORS
# with k_index and velocity + wd
autotuner_rf_wd_k <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_k, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with shannon index and velocity + wd
autotuner_rf_wd_s <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_s, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with abundance and velocity + wd
autotuner_rf_wd_a <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_a, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# set seed to obtain the same results with each run
set.seed(0412022)

# hyperparameter tuning
# VELOCITY AS PREDICTOR
# k_index and velocity 
autotuner_rf_v_k$train(task_rf_v_k)

# shannon index and velocity 
autotuner_rf_v_s$train(task_rf_v_s)

# abundance and velocity 
autotuner_rf_v_a$train(task_rf_v_a)

# VELOCITY + WD AS PREDICTORS
# k_index and velocity + wd
autotuner_rf_wd_k$train(task_rf_wd_k)

# shannon index and velocity + wd 
autotuner_rf_wd_s$train(task_rf_wd_s)

# abundance and velocity + wd
autotuner_rf_wd_a$train(task_rf_wd_a)

# read the results 
# VELOCITY AS PREDICTOR
# for k_index and velocity
autotuner_rf_v_k$tuning_result

# for shannon index and velocity
autotuner_rf_v_s$tuning_result

# for abundance and velocity
autotuner_rf_v_a$tuning_result

# VELOCITY + WD AS PREDICTORS
# for k_index and velocity + wd
autotuner_rf_wd_k$tuning_result

# for shannon index and velocity + wd
autotuner_rf_wd_s$tuning_result

# for abundance and velocity + wd
autotuner_rf_wd_a$tuning_result
```


## Predict to maps 
```{r}
# read rasters
GL1_3_28 <- stack("Pre_Processing/rasters_stacked/GL1_22_00.grd")

new_names <- c("velocity", "wd_cm")  # Replace with your desired new names
names(GL1_3_28) <- new_names
crs(GL1_3_28) <- "EPSG: 2056"

pred = terra::predict(GL1_3_28, model = autotuner_rf_wd, fun = predict, index = 2)

plot(pred)
```

vlt. zwischenschritt einbauen, Resultate vergleichen nur mit gemessenen wd und simulierten wd ein ML modell (welche Fehler macht das Modell, weil wir ihm nur simuliere Werte zum predicten geben anstatt)

Regressionsanalyse wd, v zwischen gemessen und simuliert, ich muss schon quantifizieren wie die abweichung zwischen input daten und simulierten daten sind. 
## ! Make model 
Muss ich Model noch machen? 
with training or test data? 
### Prep Data
```{r}
# TRAINING SETS
# With k_index as response variable & velocity 
training_v_k <- training_v |>
  dplyr::select(k_index, velocity)

# With shannon index as response variable & velocity 
training_v_s <- training_v |>
  dplyr::select(shannon, velocity)

# With abundance as response variable & velocity 
training_v_a <- training_v |>
  dplyr::select(abundance, velocity)

# With k_index as response variable & velocity + wd
training_wd_k <- training_wd |>
  dplyr::select(k_index, velocity, wd_cm)

# With shannon index as response variable & velocity + wd
training_wd_s <- training_wd |>
  dplyr::select(shannon, velocity, wd_cm)

# With abundance index as response variable & velocity + wd
training_wd_a <- training_wd |>
  dplyr::select(abundance, velocity, wd_cm)

# TEST SETS
# With k_index as response variable & velocity
test_v_k <- test_v |>
  dplyr::select(k_index, velocity) 

# With shannon index as response variable & velocity
test_v_s <- test_v |>
  dplyr::select(shannon, velocity) 

# With abundance as response variable & velocity
test_v_a <- test_v |>
  dplyr::select(abundance, velocity) 

# With k_index as response variable & velocity + wd
test_wd_k <- test_wd |>
  dplyr::select(k_index, velocity, wd_cm) 

# With shannon index as response variable & velocity + wd
test_wd_s <- test_wd |>
  dplyr::select(shannon, velocity, wd_cm) 

# With abundance as response variable & velocity + wd
test_wd_a <- test_wd |>
  dplyr::select(abundance, velocity, wd_cm) 
```

### Make models 
```{r}
# Create and train the random forest model with only velocity
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2059293            10          <list[4]> <list[3]> 0.1413796
rf_model_v_k <- ranger(formula = k_index ~., data = training_v_k, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

rf_model_v_s <- ranger(formula = shannon ~., data = training_v_s, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

rf_model_v_a <- ranger(formula = abundance ~., data = training_v_a, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

# Create and train the random forest model with velocity + wd
#      mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
# 1:    1       0.2059293            10          <list[4]> <list[3]>  0.141822
rf_model_wd_k <- ranger(formula = k_index ~., data = training_wd_k, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

rf_model_wd_s <- ranger(formula = shannon ~., data = training_wd_s, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

rf_model_wd_a <- ranger(formula = abundance ~., data = training_wd_a, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)


# make predictions using the trained model
# predictions <- predict(rf_model, test_data) # das stimmt bestimmt noch nicht
```

## Evaluation Metrics
### Partial dependence plots
```{r}
# Create the partial dependence plot
pdp_plot_v <- partial(rf_model_v, pred.var = "velocity")

# Plot the PDP
ggplot(pdp_plot_v, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()

# Create the partial dependence plot
pdp_plot_wd <- partial(rf_model_wd, pred.var = "wd_cm")

# Plot the PDP
ggplot(pdp_plot_wd, aes(wd_cm, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()
```


### ! Deviance explained
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions_v <- predict(rf_model_v, data = test_v)
actual_responses_v <- test_v$k_index

rf_predictions_wd <- predict(rf_model_wd, data = test_wd)
actual_responses_wd <- test_wd$k_index

# Calculate the total sum of squares (TSS)
tss_v <- sum((actual_responses_v - mean(actual_responses_v))^2)
tss_wd <- sum((actual_responses_wd - mean(actual_responses_wd))^2)

# Calculate the residual sum of squares (RSS)
rss_v <- sum((actual_responses_v - rf_predictions_v$predictions)^2)
rss_wd <- sum((actual_responses_wd - rf_predictions_wd$predictions)^2)

# Calculate R-squared
rsquared_v <- 1 - rss_v / tss_v
rsquared_wd <- 1 - rss_wd / tss_wd

# Print R-squared
print(rsquared_v)
print(rsquared_wd)
```

### ! AUC
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
rf_predictions_v <- predict(rf_model_v, data = test_v)$predictions
actual_responses_v <- test_v$k_index

rf_predictions_wd <- predict(rf_model_wd, data = test_wd)$predictions
actual_responses_wd <- test_wd$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary_v <- ifelse(actual_responses_v > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 
actual_binary_wd <- ifelse(actual_responses_wd > 0.5, 1, 0) 

# Calculate the AUC-ROC
roc_auc_v <- auc(roc(response = actual_binary_v, predictor = rf_predictions_v))
roc_auc_wd <- auc(roc(response = actual_binary_wd, predictor = rf_predictions_wd))

# Print AUC-ROC
print(roc_auc_v)
print(roc_auc_wd)
```

### ! Sensitivity & Specificity 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions_v <- predict(rf_model_v, data = test_v)$predictions
actual_responses_v <- test_v$k_index

rf_predictions_wd <- predict(rf_model_wd, data = test_wd)$predictions
actual_responses_wd <- test_wd$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold_v <- 0.5
threshold_wd <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary_v <- ifelse(actual_responses_v > threshold_v, 1, 0)
actual_binary_wd <- ifelse(actual_responses_wd > threshold_wd, 1, 0)

# Calculate confusion matrix
conf_matrix_v <- table(predicted = ifelse(rf_predictions_v > threshold_v, 1, 0), actual = actual_binary_v)
conf_matrix_wd <- table(predicted = ifelse(rf_predictions_wd > threshold_wd, 1, 0), actual = actual_binary_wd)

# Calculate sensitivity (true positive rate)
sensitivity_v <- conf_matrix_v[1, 2] / (conf_matrix_v[1, 2] + conf_matrix_v[1, 1])
sensitivity_wd <- conf_matrix_wd[1, 2] / (conf_matrix_wd[1, 2] + conf_matrix_wd[1, 1])

# Print sensitivity
print(sensitivity_v)
print(sensitivity_wd)

# Calculate specificity (true negative rate)
specificity_v <- conf_matrix_v[1, 1] / (conf_matrix_v[1, 1] + conf_matrix_v[1, 2])
specificity_wd <- conf_matrix_wd[1, 1] / (conf_matrix_wd[1, 1] + conf_matrix_wd[1, 2])

# Print specificity
print(specificity_v)
print(specificity_wd)
```

### ! Accuracy
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print accuracy
print(accuracy)
```

### ! Root mean squared deviation (RMSD)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```

### ! F - measure
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate precision
precision <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[1, 2])

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])

# Calculate F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print F1-score
print(f1_score)

```

### ! True Skill Statistic 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Calculate TSS (True Skill Statistic)
tss <- sensitivity + specificity - 1

# Print TSS
print(tss)
```

### ! CCI w/o CV
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate correctly classified instances (CCI)
cci <- conf_matrix[1, 1] + conf_matrix[2, 2]

# Print CCI
print(cci)
```

### ! Kohen's Kappa
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate observed agreement (OA)
oa <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate expected agreement (EA)
p_yes <- sum(conf_matrix[, 2]) / sum(conf_matrix)
p_no <- sum(conf_matrix[, 1]) / sum(conf_matrix)
ea <- p_yes * p_yes + p_no * p_no

# Calculate Cohen's Kappa
kappa <- (oa - ea) / (1 - ea)

# Print Cohen's Kappa
print(kappa)
```
 

# Boosted Regression Tree
## Create Task & Learner
```{r}
 # create task
task_brt_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# Create a custom learner for the 'gbm' model
custom_learner <- lrn("regr.custom_gbm", predict_type = "response")

# Define the hyperparameter search space
param_set <- ParamSet$new(
  ParamDbl("learning_rate", lower = 0.01, upper = 0.3),
  ParamInt("n_trees", lower = 50, upper = 500),
  # Add other hyperparameters here
)

# Choose a tuning method (e.g., grid search)
tune_method <- tuneParamsGrid()

# Define the resampling strategy (e.g., 5-fold cross-validation)
resampling <- rsmp("cv", folds = 5)

# Run hyperparameter tuning
tune_result <- tuneParams(custom_learner, task = your_task, resampling = resampling,
                          par.set = param_set, control = tune_method)

# Get the best model
best_model <- tune_result$learner

# Train the final model
final_model <- train(best_model, task = your_task)

# Make predictions
predictions <- predict(final_model, newdata = your_new_data)
```

### K_index & Velocity
```{r}
 # create task
task_xbrt_v <- mlr3spatiotempcv::as_task_regr_st(data_v_sp,
  id = "velocity", target = "k_index")

# learner for random forest
# from the ranger package
lrn_xbrt_v <- lrn("regr.xgboost", predict_type = "response")

# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

### K_index & Velocity + Water Depth
```{r}
 # create task
task_xbrt_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# learner for random forest
# from the ranger package
lrn_xbrt_wd <- lrn("regr.xgboost", predict_type = "response")

# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

## Specifying the search space
### K_index & Velocity
folgende Parameter sollten getuned werden: 
- bag fraction (search space = 0.5 - 0.75)
- learning rate (search space = ?)
- tree complexity (search space = ?)
```{r}

# specifying the search space
search_space_xbrt_v <- paradox::ps(
  eta = p_dbl(lower = 0.01, upper = 1.0), # learning rate
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10), # tree complexity
  subsample = p_dbl(lower = 0.5, upper = 0.75) # bag fraction 
)
```

### K_index & Velocity + Water Depth
```{r}
# specifying the search space
search_space_xbrt_wd <- paradox::ps(
  eta = p_dbl(lower = 0.01, upper = 1.0), # learning rate
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10), # tree complexity
  subsample = p_dbl(lower = 0.5, upper = 0.75) # bag fraction
)
```

## Hyperparameter tuning 
### K_index & Velocity
```{r}
autotuner_xbrt_v <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt_v,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_v, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_v$train(task_xbrt_v)

# read the results 
autotuner_xbrt_v$tuning_result
```

### K_index & Velocity + Water Depth
```{r}
autotuner_xbrt_wd <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt_wd,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_wd, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_wd$train(task_xbrt_wd)

# read the results 
autotuner_xbrt_wd$tuning_result
```

## Predict to maps 
```{r}
# read rasters
GL1_22_00 <- stack("Pre_Processing/rasters_stacked/GL1_22_00.grd")

new_names <- c("velocity", "wd_cm")  # Replace with your desired new names
names(GL1_22_00) <- new_names
crs(GL1_22_00) <- "EPSG: 2056"

pred = terra::predict(GL1_22_00, model = autotuner_xbrt_wd, fun = predict, index = 2)

plot(pred)
```

## ! Make model
```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# make training data
training.x <- model.matrix(k_index ~ ., data = train_data)

# make testing dataset
testing.x <- model.matrix(k_index ~ ., data = test_data)

# make model 
model.xgb <- xgboost(data = data.matrix(training.x[ ,-1]),
                     label = as.numeric(as.character(train_data$k_index)),
                     eta = 0.1,
                     max_depth = 3,
                     nrounds = 100,
                     objective = "reg:linear")
```

```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- as.matrix(data_new[train_indices, ])
test_data <- as.matrix(data_new[-train_indices, ])

# Create the DMatrix object
train_dmatrix <- xgb.DMatrix(data = as.matrix(train_data[, c("wd", "velocity")]), label = train_data[, "k_index"])

test_dmatrix <- xgb.DMatrix(data = as.matrix(test_data[, c("wd", "velocity")]), label = test_data[, "k_index"])

# Create and train the random forest model
xbrt_model <- xgboost(data = train_dmatrix, 
                      nrounds = 100, 
                      max_depth = 3,
                      objective = "reg:squarederror")


# make predictions using the trained model
# predictions <- predict(rf_model, test_data) # das stimmt bestimmt noch nicht

# Print the trained model
print(xbrt_model)
```

## Evaluation Metrics
### !! Partial dependence plots
```{r}
# Define the feature names
feature_names <- colnames(training.x)[2:length(colnames(training.x))]

# Generate partial dependence plots
pdp_plots <- lapply(feature_names, function(feature) {
  pdp_object <- pdp::partial(model.xgb, pred.var = feature_names, train = training.x)
  pdp_plot <- pdp::plotPartial(pdp_object, feature_names)
  return(pdp_plot)
})

# Plot the partial dependence plots
#pdf("pdp_plots.pdf")  # You can change the output format if needed
for (i in seq_along(pdp_plot)) {
  print(pdp_plot[[i]])
}
dev.off()
```

```{r}
# Create the partial dependence plot
pdp_plot_v <- partial(model.xgb, pred.var = "wd", train = train_data)

# Plot the PDP
ggplot(pdp_plot_v, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()

# Create the partial dependence plot
pdp_plot_wd <- partial(xbrt_model, pred.var = "wd")

# Plot the PDP
ggplot(pdp_plot_wd, aes(wd, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()
```

### !! Deviance explained
```{r}
# Get predictions from the XGBoost model
xbrt_predictions <- predict(xbrt_model, test_dmatrix)

# Calculate the deviance of the XGBoost predictions
deviance_xbrt <- 2 * sum(dpois(test_data[, "k_index"], xbrt_predictions, log = TRUE))

# Calculate the deviance of a null model (mean response)
mean_response <- mean(test_data[, "k_index"])
deviance_null <- 2 * sum(dpois(test_data[, "k_index"], mean_response, log = TRUE))

# Calculate the deviance explained
deviance_explained <- (deviance_null - deviance_xbrt) / deviance_null

# Print deviance explained
print(deviance_explained)
```

### !! AUC
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary <- ifelse(actual_responses > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 

# Calculate the AUC-ROC
roc_auc <- auc(roc(response = actual_binary, predictor = rf_predictions))

# Print AUC-ROC
print(roc_auc)
```

### ! Sensitivity & Specificity 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Print sensitivity
print(sensitivity)

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Print specificity
print(specificity)
```

### ! Accuracy
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print accuracy
print(accuracy)
```

### ! Root mean squared deviation (RMSD)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```

### ! F - measure
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate precision
precision <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[1, 2])

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])

# Calculate F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print F1-score
print(f1_score)

```

### ! True Skill Statistic 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Calculate TSS (True Skill Statistic)
tss <- sensitivity + specificity - 1

# Print TSS
print(tss)
```

### ! CCI w/o CV
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate correctly classified instances (CCI)
cci <- conf_matrix[1, 1] + conf_matrix[2, 2]

# Print CCI
print(cci)
```

### ! Kohen's Kappa
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate observed agreement (OA)
oa <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate expected agreement (EA)
p_yes <- sum(conf_matrix[, 2]) / sum(conf_matrix)
p_no <- sum(conf_matrix[, 1]) / sum(conf_matrix)
ea <- p_yes * p_yes + p_no * p_no

# Calculate Cohen's Kappa
kappa <- (oa - ea) / (1 - ea)

# Print Cohen's Kappa
print(kappa)
```
 