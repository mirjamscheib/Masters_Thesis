---
title: "ML_models"
format: html
---

# Libraries 
```{r}
#clear R environment
rm(list = ls())

# check for a package, install and load 
pkgCheck <- function(x){ 
  if (!require(x,character.only = TRUE)){
    install.packages(x,dependencies=TRUE)
    if(!require(x,character.only = TRUE)) {
      stop()
    }
  }
}

pkgCheck("sf")
pkgCheck("terra")
pkgCheck("dplyr")
pkgCheck("data.table")  # fast data.frame manipulation (used by mlr3)
pkgCheck("mlr3") # machine learning (see Chapter 12)
pkgCheck("mlr3spatiotempcv") # spatio-temporal resampling 
pkgCheck("mlr3tuning") # hyperparameter tuning package
pkgCheck("mlr3learners") # interface to most important machine learning packages
pkgCheck("paradox") # defining hyperparameter spaces
pkgCheck("ranger")  # rf package
pkgCheck("tree") # decision tree package
pkgCheck("readr")
pkgCheck("sp")
pkgCheck("raster")
pkgCheck("gbm")
pkgCheck("xgboost")
pkgCheck("caret") # for cohen's kappa calculation 
pkgCheck("pdp")
pkgCheck("gbm")
```


# Load data 
```{r}
# Load data containing only v, and v + wd explanatory variables
data_v <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_v.csv") 
data_wd <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_wd_v.csv") 

# w/o hydr. models "GL1", "GL2", "L2", "M1", "S1", "S2", "TH4", "VR3"
# training set containing velocity 
training_v <- data_v |>
  filter(Reach_Untersuchungsstelle != "GL1",
         Reach_Untersuchungsstelle != "GL2",         
         Reach_Untersuchungsstelle != "L2",
         Reach_Untersuchungsstelle != "M1",
         Reach_Untersuchungsstelle != "S1",
         Reach_Untersuchungsstelle != "S2",
         Reach_Untersuchungsstelle != "TH4", # noch nicht im lab datensatz, darum wirds noch nicht eingerechnet
         Reach_Untersuchungsstelle != "VR3")

# training set containing velocity + wd
training_wd <- data_wd |>
  filter(Reach_Untersuchungsstelle != "GL1",
         Reach_Untersuchungsstelle != "GL2",         
         Reach_Untersuchungsstelle != "L2",
         Reach_Untersuchungsstelle != "M1",
         Reach_Untersuchungsstelle != "S1",
         Reach_Untersuchungsstelle != "S2",
         Reach_Untersuchungsstelle != "TH4", # noch nicht im lab datensatz, darum wirds noch nicht eingerechnet
         Reach_Untersuchungsstelle != "VR3")

# w/ hydr. models 
# test set containing velocity 
test_v <- data_v |>
  filter(Reach_Untersuchungsstelle == "GL1" |
         Reach_Untersuchungsstelle == "GL2" |       
         Reach_Untersuchungsstelle == "L2" |
         Reach_Untersuchungsstelle == "M1" |
         Reach_Untersuchungsstelle == "S1" |
         Reach_Untersuchungsstelle == "S2" |
         Reach_Untersuchungsstelle == "TH4" | # fehlt noch im lab datensatz 
         Reach_Untersuchungsstelle == "VR3")

# test set containing velocity + wd
test_wd <- data_wd |>
  filter(Reach_Untersuchungsstelle == "GL1" |
         Reach_Untersuchungsstelle == "GL2" |       
         Reach_Untersuchungsstelle == "L2" |
         Reach_Untersuchungsstelle == "M1" |
         Reach_Untersuchungsstelle == "S1" |
         Reach_Untersuchungsstelle == "S2" |
         Reach_Untersuchungsstelle == "TH4" | # fehlt noch im lab datensatz 
         Reach_Untersuchungsstelle == "VR3")

# Prep data to model 
# TRAINING SETS
# With k_index as response variable & velocity 
training_v_k <- training_v |>
  dplyr::select(k_index, velocity, x, y)

# With shannon index as response variable & velocity 
training_v_s <- training_v |>
  dplyr::select(shannon, velocity, x, y)

# With abundance as response variable & velocity 
training_v_a <- training_v |>
  dplyr::select(abundance, velocity, x, y)

# With k_index as response variable & velocity + wd
training_wd_k <- training_wd |>
  dplyr::select(k_index, velocity, wd_cm, x, y)

# With shannon index as response variable & velocity + wd
training_wd_s <- training_wd |>
  dplyr::select(shannon, velocity, wd_cm, x, y)

# With abundance index as response variable & velocity + wd
training_wd_a <- training_wd |>
  dplyr::select(abundance, velocity, wd_cm, x, y)

# TEST SETS
# With k_index as response variable & velocity
test_v_k <- test_v |>
  dplyr::select(k_index, velocity, x, y) 

# With shannon index as response variable & velocity
test_v_s <- test_v |>
  dplyr::select(shannon, velocity, x, y) 

# With abundance as response variable & velocity
test_v_a <- test_v |>
  dplyr::select(abundance, velocity, x, y) 

# With k_index as response variable & velocity + wd
test_wd_k <- test_wd |>
  dplyr::select(k_index, velocity, wd_cm, x, y) 

# With shannon index as response variable & velocity + wd
test_wd_s <- test_wd |>
  dplyr::select(shannon, velocity, wd_cm, x, y) 

# With abundance as response variable & velocity + wd
test_wd_a <- test_wd |>
  dplyr::select(abundance, velocity, wd_cm, x, y) 

# convert dataframes into spatial objects for mlr3 spatial hyperparameter tuning 
# for k_index and velocity 
data_v_k_sp <-  st_as_sf(training_v_k, coords =  c("x", "y"))
st_crs(data_v_k_sp) <- "EPSG: 2056"

# for shannon index and velocity 
data_v_s_sp <-  st_as_sf(training_v_s, coords =  c("x", "y"))
st_crs(data_v_s_sp) <- "EPSG: 2056"

# for abundance and velocity 
data_v_a_sp <-  st_as_sf(training_v_a, coords =  c("x", "y"))
st_crs(data_v_a_sp) <- "EPSG: 2056"

# for k_index and velocity + wd
data_wd_k_sp <-  st_as_sf(training_wd_k, coords =  c("x", "y"))
st_crs(data_wd_k_sp) <- "EPSG: 2056"

# for shannon index and velocity + wd
data_wd_s_sp <-  st_as_sf(training_wd_s, coords =  c("x", "y"))
st_crs(data_wd_s_sp) <- "EPSG: 2056"

# for abundance and velocity + wd
data_wd_a_sp <-  st_as_sf(training_wd_a, coords =  c("x", "y"))
st_crs(data_wd_a_sp) <- "EPSG: 2056"
```


# Random Forest 
## Create Task & Learner
```{r}
# ONLY VELOCITY AS PREDICTOR (& x, y for spatial hyperparameter tuning)
# create task for k_index and velocity
task_rf_v_k <- mlr3spatiotempcv::as_task_regr_st(data_v_k_sp,
  id = "velocity", target = "k_index")

# create task for shannon index and velocity
task_rf_v_s <- mlr3spatiotempcv::as_task_regr_st(data_v_s_sp,
  id = "velocity", target = "shannon")

# create task for abundance and velocity
task_rf_v_a <- mlr3spatiotempcv::as_task_regr_st(data_v_a_sp,
  id = "velocity", target = "abundance")

# VELOCITY + WD AS PREDICTORS (& x, y for spatial hyperparameter tuning)
# create task for k_index and velocity + wd
task_rf_wd_k <- mlr3spatiotempcv::as_task_regr_st(data_wd_k_sp,
  id = "velocity", target = "k_index")

# create task for shannon index and velocity + wd
task_rf_wd_s <- mlr3spatiotempcv::as_task_regr_st(data_wd_s_sp,
  id = "velocity", target = "shannon")

# create task for abundance and velocity + wd
task_rf_wd_a <- mlr3spatiotempcv::as_task_regr_st(data_wd_a_sp,
  id = "velocity", target = "abundance")

# create learner for a random forest model from the ranger package
# for k_index, shannon index and abundance + velocity + wd valid
lrn_rf <- lrn("regr.ranger", predict_type = "response")

# performance estimation level
# perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100) # wo brauche ich das??
```

## Specifying the search space
```{r}
# ONLY VELOCITY AS PREDICTOR
# specifying the search space for k_index and velocity
search_space_v_k <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_k$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for shannon index and velocity
search_space_v_s <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_s$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for abundance and velocity
search_space_v_a <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v_a$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# VELOCITY + WD AS PREDICTORS 
# specifying the search space for k_index and velocity + wd
search_space_wd_k <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_k$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for shannon index and velocity + wd
search_space_wd_s <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_s$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# specifying the search space for abundance and velocity + wd
search_space_wd_a <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd_a$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# num.trees noch suchen? 
```

## Hyperparameter tuning 
```{r}
# VELOCITY AS PREDICTOR 
# with k_index and velocity 
autotuner_rf_v_k <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning # gibt kein richtig, falsch, je mehr folds desto mehr modelle müssen trainiert werden (rechenintensiver), 5 ist relativ üblich 
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_k, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with shannon index and velocity 
autotuner_rf_v_s <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_s, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with abundance and velocity 
autotuner_rf_v_a <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v_a, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# VELOCITY AND WD AS PREDICTORS
# with k_index and velocity + wd
autotuner_rf_wd_k <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_k, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with shannon index and velocity + wd
autotuner_rf_wd_s <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_s, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)

# with abundance and velocity + wd
autotuner_rf_wd_a <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd_a, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# set seed to obtain the same results with each run
set.seed(0412022)

# hyperparameter tuning
# VELOCITY AS PREDICTOR
# k_index and velocity 
autotuner_rf_v_k$train(task_rf_v_k)

# shannon index and velocity 
autotuner_rf_v_s$train(task_rf_v_s)

# abundance and velocity 
autotuner_rf_v_a$train(task_rf_v_a)

# VELOCITY + WD AS PREDICTORS
# k_index and velocity + wd
autotuner_rf_wd_k$train(task_rf_wd_k)

# shannon index and velocity + wd 
autotuner_rf_wd_s$train(task_rf_wd_s)

# abundance and velocity + wd
autotuner_rf_wd_a$train(task_rf_wd_a)

# read the results 
# VELOCITY AS PREDICTOR
# for k_index and velocity
autotuner_rf_v_k$tuning_result

# for shannon index and velocity
autotuner_rf_v_s$tuning_result

# for abundance and velocity
autotuner_rf_v_a$tuning_result

# VELOCITY + WD AS PREDICTORS
# for k_index and velocity + wd
autotuner_rf_wd_k$tuning_result

# for shannon index and velocity + wd
autotuner_rf_wd_s$tuning_result

# for abundance and velocity + wd
autotuner_rf_wd_a$tuning_result
```

## Make RF model 
### Prep Data w/o Coordinates
```{r}
# TRAINING SETS
# With k_index as response variable & velocity 
training_v_k <- training_v |>
  dplyr::select(k_index, velocity)

# With shannon index as response variable & velocity 
training_v_s <- training_v |>
  dplyr::select(shannon, velocity)

# With abundance as response variable & velocity 
training_v_a <- training_v |>
  dplyr::select(abundance, velocity)

# With k_index as response variable & velocity + wd
training_wd_k <- training_wd |>
  dplyr::select(k_index, velocity, wd_cm)

# With shannon index as response variable & velocity + wd
training_wd_s <- training_wd |>
  dplyr::select(shannon, velocity, wd_cm)

# With abundance index as response variable & velocity + wd
training_wd_a <- training_wd |>
  dplyr::select(abundance, velocity, wd_cm)

# TEST SETS
# With k_index as response variable & velocity
test_v_k <- test_v |>
  dplyr::select(k_index, velocity) 

# With shannon index as response variable & velocity
test_v_s <- test_v |>
  dplyr::select(shannon, velocity) 

# With abundance as response variable & velocity
test_v_a <- test_v |>
  dplyr::select(abundance, velocity) 

# With k_index as response variable & velocity + wd
test_wd_k <- test_wd |>
  dplyr::select(k_index, velocity, wd_cm) 

# With shannon index as response variable & velocity + wd
test_wd_s <- test_wd |>
  dplyr::select(shannon, velocity, wd_cm) 

# With abundance as response variable & velocity + wd
test_wd_a <- test_wd |>
  dplyr::select(abundance, velocity, wd_cm) 
```

### Make models 
```{r}
# Create and train the random forest model with only velocity
# VELOCITY AS PREDICTOR
# for k_index and velocity
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2059293            10          <list[4]> <list[3]> 0.1413796
rf_model_v_k <- ranger(formula = k_index ~., data = training_v_k, mtry = 1, sample.fraction = 0.2059293, min.node.size = 10)

# for shannon index and velocity
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2426534             7          <list[4]> <list[3]> 0.5703676
rf_model_v_s <- ranger(formula = shannon ~., data = training_v_s, mtry = 1, sample.fraction = 0.2426534, min.node.size = 7)

# for abundance and velocity
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.3763677             9          <list[4]> <list[3]>  741.6989
rf_model_v_a <- ranger(formula = abundance ~., data = training_v_a, mtry = 1, sample.fraction = 0.3763677, min.node.size = 9)

# Create and train the random forest model with velocity + wd
# VELOCITY + WD AS PREDICTORS 
# for k_index and velocity + wd
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2451874             9          <list[4]> <list[3]> 0.1371224
rf_model_wd_k <- ranger(formula = k_index ~., data = training_wd_k, mtry = 1, sample.fraction = 0.2451874, min.node.size = 9)

# for shannon index and velocity + wd
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2019869             6          <list[4]> <list[3]> 0.5556115
rf_model_wd_s <- ranger(formula = shannon ~., data = training_wd_s, mtry = 1, sample.fraction = 0.2019869, min.node.size = 6)

# for abundance and velocity + wd
#   mtry sample.fraction min.node.size learner_param_vals  x_domain regr.rmse
#1:    1       0.2068668             5          <list[4]> <list[3]>  730.9148
rf_model_wd_a <- ranger(formula = abundance ~., data = training_wd_a, mtry = 1, sample.fraction = 0.2068668, min.node.size = 5)
```

## Evaluation Metrics
### - Partial dependence plots
```{r}
# yhat ist die response variable (k-index, simpson oder shannon)
pdp_plot_v_k <- partial(rf_model_v_k, pred.var = "velocity", ice = TRUE, center = TRUE, plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", train = training_v_k, type = "regression")

# yhat ist die response variable (k-index, simpson oder shannon)
pdp_plot_wd_k <- partial(rf_model_wd_k, pred.var = c("velocity", "wd_cm"), center = TRUE, plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", train = training_wd_k, type = "regression")

# Plot the PDP
ggplot(pdp_plot_v_k, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()

# Create the partial dependence plot
pdp_plot_wd_k <- partial(rf_model_wd_k, pred.var = "wd_cm")
pdp_plot_wd_v_k <- partial(rf_model_wd_k, pred.var = "velocity")

pdp_wd <- cbind(pdp_plot_wd_k, pdp_plot_wd_v_k)

# Plot the PDP
ggplot(pdp_wd, aes(wd, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  facet_wrap(~yhat) +
  geom_line()

ggplot(pdp_plot_wd_v_k, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line() 
```


### ! Deviance explained macht Sinn
https://de.wikipedia.org/wiki/Bestimmtheitsmaß

Regressionsproblem, wenn Index/kont. Daten vorausgesagt werden
mit variante 1 rechnen 
```{r}
# # Step 1: Get predicted values
rf_predictions_v_k <- predict(rf_model_v_k, data = test_v_k)$predictions

# Step 2: Calculate TSS
mean_k <- mean(test_v_k$k_index)
TSS <- sum((test_v_k$k_index - mean_k)^2)

# Step 3: Calculate RSS
RSS <- sum((test_v_k$k_index - rf_predictions_v_k)^2)

# Step 4: Calculate R-squared
R_squared <- RSS / TSS

# Print the R-squared value
cat("R-squared (Deviance Explained):", R_squared, "\n")
```

### ! AUC nein - AUPRC?
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
rf_predictions_v_k <- predict(rf_model_v_k, data = test_v_k)$predictions
actual_responses_v_k <- test_v_k$k_index

rf_predictions_wd_k <- predict(rf_model_wd_k, data = test_wd_k)$predictions
actual_responses_wd_k <- test_wd_k$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary_v_k <- ifelse(actual_responses_v_k > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 
actual_binary_wd_k <- ifelse(actual_responses_wd_k > 0.5, 1, 0) 

# Calculate the AUC-ROC
roc_auc_v_k <- auc(roc(response = actual_binary_v_k, predictor = rf_predictions_v_k))
roc_auc_wd_k <- auc(roc(response = actual_binary_wd_k, predictor = rf_predictions_wd_k))

# Print AUC-ROC
print(roc_auc_v_k)
print(roc_auc_wd_k)
```

### ! Root mean squared deviation (RMSD) = RMSE
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model_v_k, data = test_v_k)$predictions
actual_responses <- test_v_k$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias 
keine Metric per se, Abweichung per Mittelwert (wird nicht als performance metric verwendet direkt), hat Bezug zu RMSE
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model_v_k, data = test_v_k)$predictions
actual_responses <- test_v_k$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE) ok
sieht für Regressionsproblem au
spezifisch für hydrologische Modelle 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model_v_k, data = test_v_k)$predictions
actual_responses <- test_v_k$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```
 
# Boosted Regression Tree
## Create Task & Learner
```{r}
# VELOCITY AS PREDICTOR
# for k_index and velocity 
task_xbrt_v_k <- mlr3spatiotempcv::as_task_regr_st(data_v_k_sp,
  id = "velocity", target = "k_index")

# for shannon index and velocity 
task_xbrt_v_s <- mlr3spatiotempcv::as_task_regr_st(data_v_s_sp,
  id = "velocity", target = "shannon")

# for abundance and velocity 
task_xbrt_v_a <- mlr3spatiotempcv::as_task_regr_st(data_v_a_sp,
  id = "velocity", target = "abundance")

# VELOCITY + WD AS PREDICTORS
# for k_index and velocity + wd
task_xbrt_wd_k <- mlr3spatiotempcv::as_task_regr_st(data_wd_k_sp,
  id = "water_depth", target = "k_index")

# for shannon index and velocity + wd
task_xbrt_wd_s <- mlr3spatiotempcv::as_task_regr_st(data_wd_s_sp,
  id = "water_depth", target = "shannon")

# for abundance and velocity + wd
task_xbrt_wd_a <- mlr3spatiotempcv::as_task_regr_st(data_wd_a_sp,
  id = "water_depth", target = "abundance")

# learner for random forest valid for all models
lrn_xbrt <- lrn("regr.xgboost", predict_type = "response")

# performance estimation level
# perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

```{r}
 # create task
task_xbrt_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# learner for random forest
# from the ranger package
lrn_xbrt_wd <- lrn("regr.xgboost", predict_type = "response")

# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

## Specifying the search space
folgende Parameter sollten getuned werden: 
- bag fraction (search space = 0.5 - 0.75)
- learning rate (search space = ?)
- tree complexity (search space = ?)
```{r}

# specifying the search space
search_space_xbrt_v <- paradox::ps(
  eta = p_dbl(lower = 0.01, upper = 1.0), # learning rate
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10), # tree complexity
  subsample = p_dbl(lower = 0.5, upper = 0.75) # bag fraction 
)
```

```{r}
# specifying the search space
search_space_xbrt_wd <- paradox::ps(
  eta = p_dbl(lower = 0.01, upper = 1.0), # learning rate
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10), # tree complexity
  subsample = p_dbl(lower = 0.5, upper = 0.75) # bag fraction
)
```

## Hyperparameter tuning 
```{r}
autotuner_xbrt_v_k <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_v, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_v_k$train(task_xbrt_v_k)

# read the results 
autotuner_xbrt_v_k$tuning_result
```

```{r}
autotuner_xbrt_wd <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt_wd,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_wd, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_wd$train(task_xbrt_wd)

# read the results 
autotuner_xbrt_wd$tuning_result
```

## Predict to maps 
```{r}
# read rasters
GL1_22_00 <- stack("Pre_Processing/rasters_stacked/GL1_22_00.grd")

new_names <- c("velocity", "wd_cm")  # Replace with your desired new names
names(GL1_22_00) <- new_names
crs(GL1_22_00) <- "EPSG: 2056"

pred = terra::predict(GL1_22_00, model = autotuner_xbrt_wd, fun = predict, index = 2)

plot(pred)
```

## ! Make model
```{r}
# make training data
training.x <- model.matrix(k_index ~ ., data = training_v_k)


# make testing dataset
testing.x <- model.matrix(k_index ~ ., data = test_v_k)

# make model 
# k_index and velocity
#          eta nrounds max_depth subsample learner_param_vals  x_domain regr.rmse
#1: 0.01838576     922         3 0.6312782          <list[7]> <list[4]>  0.159874
model.xgb <- xgboost(data = data.matrix(training.x[ ,-1]),
                     label = as.numeric(as.character(training_v_k$k_index)),
                     eta = 0.01838576,
                     max_depth = 3,
                     nrounds = 922,
                     subsample = 0.6312782,
                     objective = "reg:linear")
```

```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- as.matrix(data_new[train_indices, ])
test_data <- as.matrix(data_new[-train_indices, ])

# Create the DMatrix object
train_dmatrix <- xgb.DMatrix(data = as.matrix(train_data[, c("wd", "velocity")]), label = train_data[, "k_index"])

test_dmatrix <- xgb.DMatrix(data = as.matrix(test_data[, c("wd", "velocity")]), label = test_data[, "k_index"])

# Create and train the random forest model
xbrt_model <- xgboost(data = train_dmatrix, 
                      nrounds = 100, 
                      max_depth = 3,
                      objective = "reg:squarederror")


# make predictions using the trained model
# predictions <- predict(rf_model, test_data) # das stimmt bestimmt noch nicht

# Print the trained model
print(xbrt_model)
```

## Evaluation Metrics
### Partial dependence plots
https://stackoverflow.com/questions/66016542/partial-dependence-ale-ice-plots-xgboost-in-r
```{r}
 xv <- data.matrix(subset(training_v_k, select = -k_index))  # training features
# Create the partial dependence plot
pdp_plot_v_k <- partial(model.xgb, pred.var = "velocity", ice = TRUE, center = TRUE, plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", train = xv, type = "regression")
# p1xv <- partial(xgbc, pred.var = "za1", ice = TRUE, center = TRUE, plot = TRUE, rug = TRUE, alpha = 0.1, plot.engine = "ggplot2", train = xv, type = "regression")

# Plot the PDP
ggplot(pdp_plot_v_k, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()
```


### !! Deviance explained
```{r}
# Get predictions from the XGBoost model
xbrt_predictions <- predict(model.xgb, testing.x)

# Calculate the deviance of the XGBoost predictions
deviance_xbrt <- 2 * sum(dpois(testing.x[, "k_index"], xbrt_predictions, log = TRUE))

# Calculate the deviance of a null model (mean response)
mean_response <- mean(test_data[, "k_index"])
deviance_null <- 2 * sum(dpois(test_data[, "k_index"], mean_response, log = TRUE))

# Calculate the deviance explained
deviance_explained <- (deviance_null - deviance_xbrt) / deviance_null

# Print deviance explained
print(deviance_explained)
```

### !! AUC
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
brt_predictions_v <- predict(model.xgb, data = test_v_k)$predictions
actual_responses <- test_data$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary <- ifelse(actual_responses > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 

# Calculate the AUC-ROC
roc_auc <- auc(roc(response = actual_binary, predictor = rf_predictions))

# Print AUC-ROC
print(roc_auc)
```

### ! Root mean squared deviation (RMSD)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```

 