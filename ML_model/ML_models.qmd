---
title: "ML_models"
format: html
---

# Libraries 
```{r}
#clear R environment
rm(list = ls())

# check for a package, install and load 
pkgCheck <- function(x){ 
  if (!require(x,character.only = TRUE)){
    install.packages(x,dependencies=TRUE)
    if(!require(x,character.only = TRUE)) {
      stop()
    }
  }
}

pkgCheck("sf")
pkgCheck("terra")
pkgCheck("dplyr")
pkgCheck("data.table")  # fast data.frame manipulation (used by mlr3)
pkgCheck("mlr3") # machine learning (see Chapter 12)
pkgCheck("mlr3spatiotempcv") # spatio-temporal resampling 
pkgCheck("mlr3tuning") # hyperparameter tuning package
pkgCheck("mlr3learners") # interface to most important machine learning packages
pkgCheck("paradox") # defining hyperparameter spaces
pkgCheck("ranger")  # rf package
pkgCheck("tree") # decision tree package
pkgCheck("readr")
pkgCheck("sp")
pkgCheck("raster")
pkgCheck("gbm")
pkgCheck("xgboost")
pkgCheck("caret") # for cohen's kappa calculation 
pkgCheck("pdp")
pkgCheck("gbm")
```

einfachste Variante die mÃ¶glich ist test und train dataset zu trennen (hydrodyn. modelle)

# Load data 
```{r}
# Load data containing only v, and v + wd explanatory variables
data_v <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_v.csv") 
data_wd <- read_delim("Pre_Processing/abiotic_mi_sampling/lab_ml_wd_v.csv") 


# Prep data to model 
# With k_index as response variable
data_v <- data_v |>
  dplyr::select(k_index, velocity, x, y) |>
  mutate(x = as.numeric(x)) |>
  na.omit()

data_wd <- data_wd |>
  dplyr::select(k_index, velocity, wd_cm, x, y) |>
  mutate(x = as.numeric(x)) |>
  na.omit()

# convert dataframes into spatial objects 
data_v_sp <-  st_as_sf(data_v, coords =  c("x", "y"))
st_crs(data_v_sp) <- "EPSG: 2056"

data_wd_sp <-  st_as_sf(data_wd, coords =  c("x", "y"))
st_crs(data_wd_sp) <- "EPSG: 2056"
```

# Random Forest 
## Create Task & Learner
### K_index & Velocity
```{r}
# create task
task_rf_v <- mlr3spatiotempcv::as_task_regr_st(data_v_sp,
  id = "velocity", target = "k_index")

# learner for random forest
# from the ranger package
lrn_rf_v <- lrn("regr.ranger", predict_type = "response")

# performance estimation level
perf_level <- mlr3::rsmp("repeated_spcv_coords", folds = 5, repeats = 100)
```

### K_index & Velocity + Water Depth
```{r}
# create task
task_rf_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# learner for random forest
# from the ranger package
lrn_rf_wd <- lrn("regr.ranger", predict_type = "response")
```

## Specifying the search space
### K_index & Velocity
```{r}
# specifying the search space
search_space_v <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_v$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# num.trees noch suchen? 
```

### K_index & Velocity + Water Depth
```{r}
# specifying the search space
search_space_wd <- paradox::ps(
  mtry = paradox::p_int(lower = 1, upper = ncol(task_rf_wd$data()) - 1),
  sample.fraction = paradox::p_dbl(lower = 0.2, upper = 0.9),
  min.node.size = paradox::p_int(lower = 1, upper = 10)
)

# num.trees noch suchen? 
```

## Hyperparameter tuning 
### K_index & Velocity
```{r}
autotuner_rf_v <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf_v,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_v, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_rf_v$train(task_rf_v)

# read the results 
autotuner_rf_v$tuning_result
```

### K_index & Velocity + Water Depth
```{r}
autotuner_rf_wd <- mlr3tuning::AutoTuner$new(
  learner = lrn_rf_wd,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_wd, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_rf_wd$train(task_rf_wd)

# read the results 
autotuner_rf_wd$tuning_result
```

## Predict to maps 
```{r}
# read rasters
GL1_3_28 <- stack("Pre_Processing/rasters_stacked/GL1_22_00.grd")

new_names <- c("velocity", "wd_cm")  # Replace with your desired new names
names(GL1_3_28) <- new_names
crs(GL1_3_28) <- "EPSG: 2056"

pred = terra::predict(GL1_3_28, model = autotuner_rf_wd, fun = predict, index = 2)

plot(pred)
```

vlt. zwischenschritt einbauen, Resultate vergleichen nur mit gemessenen wd und simulierten wd ein ML modell (welche Fehler macht das Modell, weil wir ihm nur simuliere Werte zum predicten geben anstatt)

Regressionsanalyse wd, v zwischen gemessen und simuliert, ich muss schon quantifizieren wie die abweichung zwischen input daten und simulierten daten sind. 
## ! Make model 
```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- data_new[train_indices, ]
test_data <- data_new[-train_indices, ]

# Create and train the random forest model
rf_model <- ranger(formula = k_index ~., data = train_data, mtry = 2, sample.fraction = 0.2202553, min.node.size = 9)

# make predictions using the trained model
# predictions <- predict(rf_model, test_data) # das stimmt bestimmt noch nicht

```

## Evaluation Metrics
### Partial dependence plots
```{r}
# Create the partial dependence plot
pdp_plot_v <- partial(rf_model, pred.var = "velocity")

# Plot the PDP
ggplot(pdp_plot_v, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()

# Create the partial dependence plot
pdp_plot_wd <- partial(rf_model, pred.var = "wd")

# Plot the PDP
ggplot(pdp_plot_wd, aes(wd, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()
```


### ! Deviance explained
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)
actual_responses <- test_data$k_index

# Calculate the total sum of squares (TSS)
tss <- sum((actual_responses - mean(actual_responses))^2)

# Calculate the residual sum of squares (RSS)
rss <- sum((actual_responses - rf_predictions$predictions)^2)

# Calculate R-squared
rsquared <- 1 - rss / tss

# Print R-squared
print(rsquared)
```

### ! AUC
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary <- ifelse(actual_responses > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 

# Calculate the AUC-ROC
roc_auc <- auc(roc(response = actual_binary, predictor = rf_predictions))

# Print AUC-ROC
print(roc_auc)
```

### ! Sensitivity & Specificity 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Print sensitivity
print(sensitivity)

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Print specificity
print(specificity)
```

### ! Accuracy
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print accuracy
print(accuracy)
```

### ! Root mean squared deviation (RMSD)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```

### ! F - measure
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate precision
precision <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[1, 2])

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])

# Calculate F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print F1-score
print(f1_score)

```

### ! True Skill Statistic 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Calculate TSS (True Skill Statistic)
tss <- sensitivity + specificity - 1

# Print TSS
print(tss)
```

### ! CCI w/o CV
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate correctly classified instances (CCI)
cci <- conf_matrix[1, 1] + conf_matrix[2, 2]

# Print CCI
print(cci)
```

### ! Kohen's Kappa
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate observed agreement (OA)
oa <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate expected agreement (EA)
p_yes <- sum(conf_matrix[, 2]) / sum(conf_matrix)
p_no <- sum(conf_matrix[, 1]) / sum(conf_matrix)
ea <- p_yes * p_yes + p_no * p_no

# Calculate Cohen's Kappa
kappa <- (oa - ea) / (1 - ea)

# Print Cohen's Kappa
print(kappa)
```
 

# Boosted Regression Tree
## Create Task & Learner
```{r}
 # create task
task_brt_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# Create a custom learner for the 'gbm' model
custom_learner <- lrn("regr.custom_gbm", predict_type = "response")

# Define the hyperparameter search space
param_set <- ParamSet$new(
  ParamDbl("learning_rate", lower = 0.01, upper = 0.3),
  ParamInt("n_trees", lower = 50, upper = 500),
  # Add other hyperparameters here
)

# Choose a tuning method (e.g., grid search)
tune_method <- tuneParamsGrid()

# Define the resampling strategy (e.g., 5-fold cross-validation)
resampling <- rsmp("cv", folds = 5)

# Run hyperparameter tuning
tune_result <- tuneParams(custom_learner, task = your_task, resampling = resampling,
                          par.set = param_set, control = tune_method)

# Get the best model
best_model <- tune_result$learner

# Train the final model
final_model <- train(best_model, task = your_task)

# Make predictions
predictions <- predict(final_model, newdata = your_new_data)
```

### K_index & Velocity
```{r}
 # create task
task_xbrt_v <- mlr3spatiotempcv::as_task_regr_st(data_v_sp,
  id = "velocity", target = "k_index")

# learner for random forest
# from the ranger package
lrn_xbrt_v <- lrn("regr.xgboost", predict_type = "response")
```

### K_index & Velocity + Water Depth
```{r}
 # create task
task_xbrt_wd <- mlr3spatiotempcv::as_task_regr_st(data_wd_sp,
  id = "water_depth", target = "k_index")

# learner for random forest
# from the ranger package
lrn_xbrt_wd <- lrn("regr.xgboost", predict_type = "response")
```

## Specifying the search space
### K_index & Velocity
folgende Parameter sollten getuned werden: 
- bag fraction (search space = 0.5 - 0.75)
- learning rate (search space = ?)
- tree complexity (search space = ?)
```{r}

# specifying the search space
search_space_xbrt_v <- paradox::ps(
  #learning_rate = p_dbl(lower = 0.01, upper = 0.5),
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10)
)
```

### K_index & Velocity + Water Depth
```{r}
# specifying the search space
search_space_xbrt_wd <- paradox::ps(
  #learning_rate = p_dbl(lower = 0.01, upper = 0.5),
  nrounds = p_int(lower = 100, upper = 1000),
  max_depth = p_int(lower = 3, upper = 10)
)
```

## Hyperparameter tuning 
### K_index & Velocity
```{r}
autotuner_xbrt_v <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt_v,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_v, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_v$train(task_xbrt_v)

# read the results 
autotuner_xbrt_v$tuning_result
```

### K_index & Velocity + Water Depth
```{r}
autotuner_xbrt_wd <- mlr3tuning::AutoTuner$new(
  learner = lrn_xbrt_wd,
  resampling = mlr3::rsmp("spcv_coords", folds = 5), # spatial partitioning
  measure = mlr3::msr("regr.rmse"), # performance measure
  terminator = mlr3tuning::trm("evals", n_evals = 50), # specify 50 iterations / run 50 models
  search_space = search_space_xbrt_wd, # predefined hyperparameter search space
  tuner = mlr3tuning::tnr("random_search") # specify random search
)
```

```{r}
# hyperparameter tuning
set.seed(0412022)
autotuner_xbrt_wd$train(task_xbrt_wd)

# read the results 
autotuner_xbrt_wd$tuning_result
```

## Predict to maps 
```{r}
# read rasters
GL1_22_00 <- stack("Pre_Processing/rasters_stacked/GL1_22_00.grd")

new_names <- c("velocity", "wd_cm")  # Replace with your desired new names
names(GL1_22_00) <- new_names
crs(GL1_22_00) <- "EPSG: 2056"

pred = terra::predict(GL1_22_00, model = autotuner_xbrt_wd, fun = predict, index = 2)

plot(pred)
```

## ! Make model
```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# make training data
training.x <- model.matrix(k_index ~ ., data = train_data)

# make testing dataset
testing.x <- model.matrix(k_index ~ ., data = test_data)

# make model 
model.xgb <- xgboost(data = data.matrix(training.x[ ,-1]),
                     label = as.numeric(as.character(train_data$k_index)),
                     eta = 0.1,
                     max_depth = 3,
                     nrounds = 100,
                     objective = "reg:linear")
```

```{r}
# turn sf object into dataframe 
data_new <- data.frame(st_drop_geometry(data_new))

# Split the data into training and testing sets
train_indices <- sample(1:nrow(data_new), 0.7 * nrow(data_new))
train_data <- as.matrix(data_new[train_indices, ])
test_data <- as.matrix(data_new[-train_indices, ])

# Create the DMatrix object
train_dmatrix <- xgb.DMatrix(data = as.matrix(train_data[, c("wd", "velocity")]), label = train_data[, "k_index"])

test_dmatrix <- xgb.DMatrix(data = as.matrix(test_data[, c("wd", "velocity")]), label = test_data[, "k_index"])

# Create and train the random forest model
xbrt_model <- xgboost(data = train_dmatrix, 
                      nrounds = 100, 
                      max_depth = 3,
                      objective = "reg:squarederror")


# make predictions using the trained model
# predictions <- predict(rf_model, test_data) # das stimmt bestimmt noch nicht

# Print the trained model
print(xbrt_model)
```

## Evaluation Metrics
### !! Partial dependence plots
```{r}
# Define the feature names
feature_names <- colnames(training.x)[2:length(colnames(training.x))]

# Generate partial dependence plots
pdp_plots <- lapply(feature_names, function(feature) {
  pdp_object <- pdp::partial(model.xgb, pred.var = feature_names, train = training.x)
  pdp_plot <- pdp::plotPartial(pdp_object, feature_names)
  return(pdp_plot)
})

# Plot the partial dependence plots
#pdf("pdp_plots.pdf")  # You can change the output format if needed
for (i in seq_along(pdp_plot)) {
  print(pdp_plot[[i]])
}
dev.off()
```

```{r}
# Create the partial dependence plot
pdp_plot_v <- partial(model.xgb, pred.var = "wd", train = train_data)

# Plot the PDP
ggplot(pdp_plot_v, aes(velocity, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()

# Create the partial dependence plot
pdp_plot_wd <- partial(xbrt_model, pred.var = "wd")

# Plot the PDP
ggplot(pdp_plot_wd, aes(wd, yhat)) + 
  # yhat ist die response variable (k-index, simpson oder shannon)
  geom_line()
```

### !! Deviance explained
```{r}
# Get predictions from the XGBoost model
xbrt_predictions <- predict(xbrt_model, test_dmatrix)

# Calculate the deviance of the XGBoost predictions
deviance_xbrt <- 2 * sum(dpois(test_data[, "k_index"], xbrt_predictions, log = TRUE))

# Calculate the deviance of a null model (mean response)
mean_response <- mean(test_data[, "k_index"])
deviance_null <- 2 * sum(dpois(test_data[, "k_index"], mean_response, log = TRUE))

# Calculate the deviance explained
deviance_explained <- (deviance_null - deviance_xbrt) / deviance_null

# Print deviance explained
print(deviance_explained)
```

### !! AUC
```{r}
# Install and load the required package
# install.packages("pROC")
library(pROC)

# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Transform the actual responses into a binary indicator (0 or 1)
actual_binary <- ifelse(actual_responses > 0.5, 1, 0)  # Set the appropriate threshold
# threshold im moment 0.5 aber nachher noch anpassen vlt. 

# Calculate the AUC-ROC
roc_auc <- auc(roc(response = actual_binary, predictor = rf_predictions))

# Print AUC-ROC
print(roc_auc)
```

### ! Sensitivity & Specificity 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Print sensitivity
print(sensitivity)

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Print specificity
print(specificity)
```

### ! Accuracy
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate accuracy
accuracy <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Print accuracy
print(accuracy)
```

### ! Root mean squared deviation (RMSD)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate squared deviations
squared_deviations <- (rf_predictions - actual_responses)^2

# Calculate mean of squared deviations
mean_squared_deviation <- mean(squared_deviations)

# Calculate root mean squared deviation (RMSE)
root_mean_squared_deviation <- sqrt(mean_squared_deviation)

# Print RMSE
print(root_mean_squared_deviation)
```

### ! Bias
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the differences between predicted and actual values
differences <- rf_predictions - actual_responses

# Calculate the mean of the differences (bias)
bias <- mean(differences)

# Print bias
print(bias)
```

### ! Nash-Sutcliffe efficiency (NSE)
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Calculate the NSE
nse <- 1 - sum((actual_responses - rf_predictions)^2) / sum((actual_responses - mean(actual_responses))^2)

# Print NSE
print(nse)
```

### ! F - measure
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate precision
precision <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[1, 2])

# Calculate recall (sensitivity)
recall <- conf_matrix[2, 2] / (conf_matrix[2, 2] + conf_matrix[2, 1])

# Calculate F1-score
f1_score <- 2 * (precision * recall) / (precision + recall)

# Print F1-score
print(f1_score)

```

### ! True Skill Statistic 
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate sensitivity (true positive rate)
sensitivity <- conf_matrix[1, 2] / (conf_matrix[1, 2] + conf_matrix[1, 1])

# Calculate specificity (true negative rate)
specificity <- conf_matrix[1, 1] / (conf_matrix[1, 1] + conf_matrix[1, 2])

# Calculate TSS (True Skill Statistic)
tss <- sensitivity + specificity - 1

# Print TSS
print(tss)
```

### ! CCI w/o CV
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate correctly classified instances (CCI)
cci <- conf_matrix[1, 1] + conf_matrix[2, 2]

# Print CCI
print(cci)
```

### ! Kohen's Kappa
```{r}
# Assuming 'rf_model' is your trained random forest model
rf_predictions <- predict(rf_model, data = test_data)$predictions
actual_responses <- test_data$k_index

# Assuming you have set an appropriate threshold to classify positive and negative cases
threshold <- 0.5

# Transform the actual responses into binary indicators (0 or 1)
actual_binary <- ifelse(actual_responses > threshold, 1, 0)

# Calculate confusion matrix
conf_matrix <- table(predicted = ifelse(rf_predictions > threshold, 1, 0), actual = actual_binary)

# Calculate observed agreement (OA)
oa <- sum(diag(conf_matrix)) / sum(conf_matrix)

# Calculate expected agreement (EA)
p_yes <- sum(conf_matrix[, 2]) / sum(conf_matrix)
p_no <- sum(conf_matrix[, 1]) / sum(conf_matrix)
ea <- p_yes * p_yes + p_no * p_no

# Calculate Cohen's Kappa
kappa <- (oa - ea) / (1 - ea)

# Print Cohen's Kappa
print(kappa)
```
 