---
title: "BRT_Model_Elith_2008"
format: html
---

# Boosted Regression Trees in R

# Install BRT functions & packages
```{r}
source("brt.functions.R")
library("gbm")
library(dismo)
library(dplyr)
library(raster)
library(readr)
```

# Importing example data & data

```{r}
# example data
model.data <- read.csv("Data/model.data.csv")

# make factor 
model.data$Method <- as.factor(model.data$Method)

# Load Data
data <- read.csv("Data/Data_Complete/Lab_k_index.csv")

# choose only relevant variables 
data <- data[ ,c(7, 9,27)] # 22:24 = x, y, z

data <- data |>
  rename(wd_cm = Water_depth_cm, v_cm_s = `Flow_velocity_v60_cm.s`) |>
  mutate(wd_cm = wd_cm/100)

head(data)
```

```{r}
# Split the data into training and testing sets
train_indices <- sample(1:nrow(data), 0.7 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
```

# Fitting a model
```{r}
# You need to decide what settings to use ? the article associated with this tutorial gives you information on what to use as rules of thumb. 
# These data have 1000 sites, comprising 202 presence records for the short-finned eel (the command sum(model.data$Angaus)will give you the total number of presences). As a first guess you could decide there are enough data to model interactions of reasonable complexity, and a lr of about 0.01 could be a reasonable starting point.
# To use our function that steps forward and identifies the optimal number of trees (nt) use this call:

angaus.tc5.lr01 <- gbm.step(data=model.data, 
    gbm.x = 3:14, 
    gbm.y = 2,
    family = "bernoulli",
    tree.complexity = 5,
    learning.rate = 0.01,
    bag.fraction = 0.5)

# geht noch nicht
data_l2 <- gbm.step(data= data, 
    gbm.x = 1:2,
    gbm.y = 3,
    family = "laplace",
    tree.complexity = 2,
    learning.rate = 0.002,
    bag.fraction = 0.5)
    
# Note that this function is an alternative to the cross-validation one that Ridgeway provides). Here we are using the function gbm.step and have passed information to the function about data and settings. We explain what we have defined in the full word version of this tutorial

# Firstly, the things you can see: 
# The R console will show some results ? see the Word version of this tutorial for an example. It reports a brief model summary ? all the values are also retained in the model object, so they will be permanently kept (as long as you save the R workspace before quitting).

# There will also be a graph..

# This model was built with the default 10-fold cross-validation (CV) ? the solid black curve is the mean, and the dotted curves ? 1 standard error, for the changes in predictive deviance (ie as measured on the excluded folds of the CV). The red line shows the minimum of the mean, and the green line the number of trees at which that occurs. The final model that is returned in the model object is built on the full data set, using the number of trees identified as optimal.
```

```{r}
# The returned object is a list (see R documentation if you don?t know what that is), and the names of the components can be seen by typing:
names(angaus.tc5.lr01)

# To pull out one component of the list, use a number (angaus.tc5.lr01[[29]]) or name (angaus.tc5.lr01$cv.statistics) ? but be careful ? some are as big as the dataset ? e.g. there will be 1000 fitted values ? find this by typing length(angaus.tc5.lr01$fitted)

(angaus.tc5.lr01$cv.statistics)
```

# Choosing the settings
```{r}
# The above was a first guess at settings, using rules of thumb discussed in Elith et al. (2008). It made a model with only 650 trees, so our next step would be to reduce the lr  - e.g., try lr = 0.005, to aim for over 1000 trees ? i.e.:

angaus.tc5.lr005 <- gbm.step(data=model.data, 
    gbm.x = 3:14, # auch hier wÃ¤re 3:14 aber hat irgendwie probleme 
    gbm.y = 2,
    family = "bernoulli",
    tree.complexity = 5,
    learning.rate = 0.005,
    bag.fraction = 0.5)

data_l2.x <- gbm.step(data= data, 
    gbm.x = 1:2,
    gbm.y = 3,
    family = "laplace",
    tree.complexity = 5,
    learning.rate = 0.00005,
    bag.fraction = 0.5)

# To more broadly explore whether other settings perform better, and assuming that these are the only data available, you could either split the data into a training and testing set or use the CV results. You could systematically alter tc,  lr and the bag fraction and compare the results. See the later section on prediction to find out how to predict to independent data and calculate relevant statistics. 
```


# Simplifying the model
```{r}
# For a discussion of simplification see Appendix 2 of the online supplement to Elith et al (2008). Simplification builds many models, so it can be slow. For example, the code below took a few minutes to run on a modern laptop. In it we assess the value in simplifying the model built with a lr of 0.005, but only test dropping up to 5 variables (the "n.drop" argument; the default is an automatic rule so it continues until the average change in predictive deviance exceeds its original standard error as calculated in gbm.step).

angaus.simp <- gbm.simplify(angaus.tc5.lr005, n.drops = 5)

data.simp <- gbm.simplify(data_l2, n.drops = 5)

# For our run, this estimated that the optimal number of variables to drop was 1; yours could be slightly different. You can use the number indicated by the red vertical line, or look at the results in the angaus.simp object

# Now make a model with 1 predictor dropped, by indicating to the gbm.step call the relevant number of predictor(s) from the predictor list in the angaus.simp object ? see highlights, below, in which we indicate we want to drop 1 variable by calling the second vector of predictor columns in the pred list, using a [[1]]:

angaus.tc5.lr005.simp <- gbm.step(model.data, gbm.x = angaus.simp$pred.list[[1]], gbm.y = 2, tree.complexity = 5, learning.rate = 0.005)

data.simp <- gbm.step(data, gbm.x = data.simp$pred.list[[1]], gbm.y = 2, tree.complexity = 5, learning.rate = 0.00005)

# This has now made a new model (angaus.tc5.lr005.simp) with the same list components as described earlier. We could continue to use it, but given that we don't particularly want a more simple model (our view is that, in a dataset of this size, included variables that contribute little are acceptable), we won't use it further.
```


# Keeping the workspace at a reasonable size
```{r}
# We find that the R workspaces can get very large with BRT models, and that the usual cleanup procedures don't reduce their size. Dumping the models out of the workspace and re-reading them in fixes the problem. If your models all had "angaus" in the name, this code would work:

dump(ls(pattern="angaus"), "scratch.R") 
source("scratch.R")
save.image("C:/brt/.RData")
```

# Plotting the functions and fitted values from the model
```{r}
# The fitted functions from a BRT model created from any of our functions can be plotted using gbm.plot. If you want to plot all variables on one sheet first set up a graphics device with the right set-up  - here we will make one with 3 rows and 4 columns:	
					
par(mfrow=c(2,1))
gbm.plot(angaus.tc5.lr005, n.plots=12, write.title = F)

gbm.plot(data_l2, n.plots = 5, write.title = F)


# Additional arguments to this function allow for making a smoothed representation of the plot, allowing different vertical scales for each variable, omitting (and formatting) the rugs, and plotting a single variable. 

# Depending on the distribution of observations within the environmental space, fitted functions can give a misleading indication about the distribution of the fitted values in relation to each predictor. The function gbm.plot.fits has been provided to plot the fitted values in relation to each of the predictors used in the model. 

gbm.plot.fits(angaus.tc5.lr005)

gbm.plot.fits(data_l2)

# This has options that allow for the plotting of all fitted values or of fitted values only for positive observations, or the plotting of fitted values in factor type graphs that are much quicker to print. Values above each graph indicate the weighted mean of fitted values in relation to each non-factor predictor.
```

# Interrogate and plot the interactions
```{r}
# This code assesses the extent to which pairwise interactions exist in the data.

find.int <- gbm.interactions(angaus.tc5.lr005)

find.int.data <- gbm.interactions(data_l2)

# The returned object, here named test.int, is a list. The first 2 components summarise the results, first as a ranked list of the 5 most important pairwise interactions, and the second tabulating all pairwise interactions. The variable index numbers in $rank.list can be used for plotting.

# You can plot pairwise interactions like this:

gbm.perspec(angaus.tc5.lr005,7,1,y.range = c(15,20), z.range=c(0,0.6))

gbm.perspec(data_l2,1,2,y.range = c(15,20), z.range=c(0,0.6))

# Additional options allow specifications of label names, rotations of the 3D graph and so on.
```

# Predicting to new data
```{r}
# We provide code for making predictions, and this has an option for outputting gridded maps of predictions.
# If you want to predict to a set of sites (rather than to a whole map), the general procedure is to set up a data frame with rows for sites and columns for the variables that are in your model. R is case sensitive; the names need to exactly match those in the model. Other columns such as site ids etc can also exist in the dataframe. 
# Our dataset for predicting to sites is in a file called eval.data.csv ? read it in:

eval.data <- read.csv("eval.data.csv", as.is=T)

# The "Method" column needs to be converted to a factor, with levels matching those in the modelling data:

eval.data$Method <- factor(eval.data$Method, levels = levels(model.data$Method))

# To make predictions to sites from the BRT model use Ridgeway's code:

library(gbm)
preds <- predict.gbm(angaus.tc5.lr005, eval.data, n.trees=angaus.tc5.lr005$gbm.call$best.trees, type="response")

# or use our code with the defaults, which means it just makes predictions to the workspace:

gbm.predict.grids(angaus.tc5.lr005, eval.data, want.grids = F, sp.name = "preds")

# In both cases the predictions will be in a vector called preds, because that's what we named them.
# These are evaluation sites, and have observations in column 1 (named Angaus_obs). They are independent of the model building set and could be used for an independent evaluation. For example here is code for calculating the deviance and the AUC (area under the ROC curve):

calc.deviance(eval.data$Angaus_obs,preds,calc.mean=T)
roc(eval.data$Angaus_obs,preds)

# Note that the calc.deviance function has different formulae for different distributions of data; the default is binomial, so we didn't specify it in the call
```


# Code for a figure like figure 2, Elith et al. (2008)
```{r}
# One useful feature of prediction in gbm is you can predict to a varying number of trees.
# The full set of code here shows how to make one of the graphed lines from Fig. 2 in our paper, using a model of 5000 trees developed with gbm.fixed

angaus.5000 <- gbm.fixed(data=model.data, gbm.x = 3:14, gbm.y = 2, learning.rate = 0.005, tree.complexity = 5, n.trees = 5000)

tree.list <- seq(100, 5000, by = 100)

pred <- predict.gbm(angaus.5000, eval.data, n.trees = tree.list, "response")

# Note that the code above makes a matrix, with each column being the predictions from the model angaus.5000 to the number of trees specified by that element of tree.list ? for example, the predictions in column 5 are for tree.list[5] = 500 trees. 
# Now to calculate the deviance of all these results, and plot them:

angaus.pred.deviance <- rep(0,50)

for (i in 1:50) {
angaus.pred.deviance[i] <- calc.deviance(eval.data$Angaus_obs, pred[,i],calc.mean=T)
}

plot(tree.list,angaus.pred.deviance,ylim=c(0.7,1),xlim = c(-100,5000),type='l', xlab = "number of trees",ylab = "predictive deviance", cex.lab = 1.5) 

```

# Making grids
```{r}
# Alternatively, you can predict to grids (i.e., to a whole map). To do this, export the grids for your variables of interest from a GIS program, as ascii grids (note - these must all have the same cell size and the same number of rows and columns). We provide our data as ascii grids, plus a mask, that has "1" for a river and nodata elsewhere. These are rasterized versions of the line data we usually work with. You can view them by importing them into a GIS program. You can alternatively view just the header by opening the ascii file a text reading program; the first six lines contain information you will need if you want to make a grid of predictions. 
# Import them into R:
grid.names <- c("mask.asc", "segsumt.asc", "segtseas.asc","seglowflow.asc","dsdist.asc","dsmaxslope.asc","usavgt.asc","usraindays.asc","usslope.asc", "usnative.asc", "dsdam.asc", "locsed.asc")

variable.names <- c("mask", names(model.data)[c(3:12,14)]) # here make sure the order is the same as above, if you're using different data

for(i in 1:length(grid.names)){
assign(variable.names[i],scan(grid.names[i], skip=6, na.string = "-9999"), pos=1)
}

# Make them into a data frame, adding a column for the Method ? we will predict the probability of catch using electric fishing:

preddat <- data.frame (SegSumT,USNative,DSDist,LocSed,DSMaxSlope,USSlope,USRainDays,USAvgT,SegTSeas,SegLowFlow,DSDam,rep("electric", length(SegSumT)))

names(preddat)[12] <- "Method"

preddat$Method <- factor(preddat$Method, levels = levels(model.data$Method))

# This data frame has 49000 rows, because there were 49000 cells in each grid. Whilst you could predict to all sites, for very large grids you might want to reduce it to the sites you are interested in:

preddat<- preddat[!is.na(mask),]

# You will now have have 8058 rows in the data.
#gbm will predict to sites even if there is no data there, so in your own data make sure you mask out those sites with no data, or for which there are insufficient predictors. Our code will re-form these reduced data frames into a full grid, so long as you form the data frame first then reduce it (as above) and give the function a vector the original length of one of the scanned grids. This is how to make a grid (it also returns the predictions to the R workspace too):

gbm.predict.grids(angaus.tc5.lr005, preddat, want.grids = T, sp.name = "angaus_preds",pred.vec = rep(-9999,49000), filepath = "c:/brt/", num.col = 250, num.row = 196, xll = 0, yll = 0, cell.size = 100, no.data = -9999, plot=T) 

# The information from the header file is included here (highlighted in aqua), and the pred.vec argument is where you give information on the value to be NO DATA (here, -9999) and the number of grid cells in the original grids (here, 49000). It will make a map in R. It will also have made a file in the directory you specified ? here, it will be c:/brt/angaus_preds.csv (see yellow highlights). This could be read into in a GIS program.
```

# Dealing with large grids
```{r}
# If you have very large files you can do the above in a loop. For example, pretending that these ascii files are large and that even with changes to the memory size in R you can't predict, our code can be used in a loop. Let's say we have to do it in 4 repetitions; you would do a first run to make an output with a header file, then do the rest in a loop:

# First detail how many rows you will have in each run; here we'll do ? at a time

rep.rows <- c(49,49,49,49)

# Then do the first run:

for(i in 1:length(grid.names)){
assign(variable.names[i],scan(grid.names[i], skip=6, na.string = "-9999", nlines = rep.rows[1]), pos=1)
}

preddat <- data.frame (SegSumT,USNative,DSDist,LocSed,DSMaxSlope,USSlope,USRainDays,USAvgT,SegTSeas,SegLowFlow,DSDam,rep("electric", length(SegSumT)))

names(preddat)[12] <- "Method"

preddat$Method <- factor(preddat$Method, levels = levels(model.data$Method))

preddat<- preddat[!is.na(mask),]

# This version of preddat (above) only has 2116 rows, and before we masked it it was 12250 rows. You can use our code similar to before; it will write the header file, plus the predictions to these 2116 rows. We highlight the new parts of the commands, for clarity

gbm.predict.grids(angaus.tc5.lr005, preddat, want.grids = T, sp.name = "angaus_preds2",pred.vec = rep(-9999,250 * rep.rows[1]), filepath = "c:/brt/", num.col = 250, num.row = 196, xll = 0, yll = 0, cell.size = 100, no.data = -9999, plot=T, full.grid = F, part.number = 1, part.row = rep.rows[1]) 

# Then do the rest in a loop:

for(i in 2:4){
for(j in 1:length(grid.names)){
assign(variable.names[j],scan(grid.names[j], skip=(6 + sum(rep.rows[1:(i-1)])), na.string = "-9999", nlines = rep.rows[i]), pos=1)
}

preddat <- data.frame (SegSumT,USNative,DSDist,LocSed,DSMaxSlope,USSlope,USRainDays,USAvgT,SegTSeas,SegLowFlow,DSDam,rep("electric", length(SegSumT)))
names(preddat)[12] <- "Method"
preddat$Method <- factor(preddat$Method, levels = levels(model.data$Method))
preddat<- preddat[!is.na(mask),]

gbm.predict.grids(angaus.tc5.lr005, preddat, want.grids = T, sp.name = "angaus_preds2",pred.vec = rep(-9999,250 * rep.rows[i]), filepath = "c:/brt/", num.col = 250, full.grid = F, part.number = i, part.row = rep.rows[i], header = F) 
}

# Note that the function is currently also saving the predictions in the R workspace ? for large grids you probably want to turn that off; use pred2R = F within the call above.

```




